{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gengliu/anaconda3/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (3.0.4)/charset_normalizer (3.3.2) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "from bert_score import score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import itertools\n",
    "# Suppress loading messages\n",
    "transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.modeling_utils.logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib configuration\n",
    "rcParams[\"xtick.major.size\"] = 0\n",
    "rcParams[\"xtick.minor.size\"] = 0\n",
    "rcParams[\"ytick.major.size\"] = 0\n",
    "rcParams[\"ytick.minor.size\"] = 0\n",
    "rcParams[\"axes.labelsize\"] = \"large\"\n",
    "rcParams[\"axes.axisbelow\"] = True\n",
    "rcParams[\"axes.grid\"] = True\n",
    "\n",
    "# Load data\n",
    "baidu = pd.read_csv(\"../data/baidu_v5.csv\")\n",
    "ernie = pd.read_csv(\"../data/ernie_v6.csv\")\n",
    "qwen = pd.read_csv(\"../data/qwen_v6.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_similarity_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m ernie_groups \u001b[38;5;241m=\u001b[39m ernie[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup_English\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m      3\u001b[0m qwen_groups \u001b[38;5;241m=\u001b[39m qwen[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup_English\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m----> 5\u001b[0m similarity_df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_similarity_dataframe\u001b[49m(ernie_groups)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_similarity_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "# # Create similarity dataframe for ernie groups\n",
    "# ernie_groups = ernie['group_English'].unique()\n",
    "# qwen_groups = qwen['group_English'].unique()\n",
    "\n",
    "# similarity_df = create_similarity_dataframe(ernie_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create similarity dataframe (if grp1, grp2 is present, do not calculate grp2, grp1)\n",
    "def create_similarity_dataframe(groups):\n",
    "    combinations = [e for e in itertools.combinations(ernie_groups, 2)]\n",
    "    return pd.DataFrame(combinations, columns=['group1', 'group2'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ernie_groups = ernie['group_English'].unique()\n",
    "similarity_df = create_similarity_dataframe(ernie_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'similarity_df_qwen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msimilarity_df_qwen\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'similarity_df_qwen' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28882293 3230466\n"
     ]
    }
   ],
   "source": [
    "# Create similarity dataframe for ernie groups\n",
    "\n",
    "\n",
    "ones = []\n",
    "twos = []\n",
    "\n",
    "# all the combinations for all the possible groups\n",
    "for index, row in similarity_df.iterrows():\n",
    "    print(index, end='\\r')\n",
    "    group1_responses = ernie[ernie['group_English'] ==\n",
    "                             row['group1']]['json_response'].dropna().tolist()\n",
    "    group2_responses = ernie[ernie['group_English'] ==\n",
    "                             row['group2']]['json_response'].dropna().tolist()\n",
    "    if group1_responses and group2_responses:\n",
    "        elems = [e for e in itertools.product(\n",
    "            group1_responses, group2_responses)]\n",
    "        one = [e[0] for e in elems]\n",
    "        two = [e[1] for e in elems]\n",
    "        ones.extend(one)\n",
    "        twos.extend(two)\n",
    "\n",
    "# calculate each bert score only once! (you can map the scores later to \"correspondences\" df)\n",
    "correspondences = pd.DataFrame({'ones': ones, 'twos': twos})\n",
    "unique_correspondences = correspondences.drop_duplicates()\n",
    "\n",
    "print(len(correspondences), len(unique_correspondences))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ones</th>\n",
       "      <th>twos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>努力追求成功</td>\n",
       "      <td>努力</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>努力追求成功</td>\n",
       "      <td>坚强</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>努力追求成功</td>\n",
       "      <td>拼搏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>努力追求成功</td>\n",
       "      <td>有毅力</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>努力追求成功</td>\n",
       "      <td>上进</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28882288</th>\n",
       "      <td>有力量</td>\n",
       "      <td>勇于进取</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28882289</th>\n",
       "      <td>有力量</td>\n",
       "      <td>团结一心</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28882290</th>\n",
       "      <td>有力量</td>\n",
       "      <td>勤劳能干</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28882291</th>\n",
       "      <td>有力量</td>\n",
       "      <td>善于学习</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28882292</th>\n",
       "      <td>有力量</td>\n",
       "      <td>积极进取</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28882293 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ones  twos\n",
       "0         努力追求成功    努力\n",
       "1         努力追求成功    坚强\n",
       "2         努力追求成功    拼搏\n",
       "3         努力追求成功   有毅力\n",
       "4         努力追求成功    上进\n",
       "...          ...   ...\n",
       "28882288     有力量  勇于进取\n",
       "28882289     有力量  团结一心\n",
       "28882290     有力量  勤劳能干\n",
       "28882291     有力量  善于学习\n",
       "28882292     有力量  积极进取\n",
       "\n",
       "[28882293 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correspondences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76410d1c31c6472ba6b6efca1a71a8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ee3d8616f44860b7c2c7da90fd7b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50477 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2428.63 seconds, 1330.16 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "unique_ones = unique_correspondences.ones.tolist()\n",
    "unique_twos = unique_correspondences.twos.tolist()\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "P_scores, R_scores, F1_scores = score(\n",
    "    unique_ones, unique_twos, lang='zh', verbose=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x1/rg74ttxx7bb0_2z1wxpb2z940000gn/T/ipykernel_2101/2612561919.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_correspondences['P_score'] = P_scores_list\n",
      "/var/folders/x1/rg74ttxx7bb0_2z1wxpb2z940000gn/T/ipykernel_2101/2612561919.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_correspondences['R_score'] = R_scores_list\n",
      "/var/folders/x1/rg74ttxx7bb0_2z1wxpb2z940000gn/T/ipykernel_2101/2612561919.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_correspondences['F1_score'] = F1_scores_list\n"
     ]
    }
   ],
   "source": [
    "# Assuming P_scores, R_scores, and F1_scores are tensors or similar structures, convert them to lists\n",
    "P_scores_list = P_scores.tolist()\n",
    "R_scores_list = R_scores.tolist()\n",
    "F1_scores_list = F1_scores.tolist()\n",
    "\n",
    "# Add these lists as new columns in the DataFrame\n",
    "unique_correspondences['P_score'] = P_scores_list\n",
    "unique_correspondences['R_score'] = R_scores_list\n",
    "unique_correspondences['F1_score'] = F1_scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_correspondences.to_csv('ernie_bert_unique_correspondences.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Qwen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40711855 2583861\n"
     ]
    }
   ],
   "source": [
    "# Create similarity dataframe for ernie groups\n",
    "\n",
    "\n",
    "ones = []\n",
    "twos = []\n",
    "\n",
    "# all the combinations for all the possible groups\n",
    "for index, row in similarity_df.iterrows():\n",
    "    print(index, end='\\r')\n",
    "    group1_responses = qwen[qwen['group_English'] ==\n",
    "                             row['group1']]['json_response'].dropna().tolist()\n",
    "    group2_responses = qwen[qwen['group_English'] ==\n",
    "                             row['group2']]['json_response'].dropna().tolist()\n",
    "    if group1_responses and group2_responses:\n",
    "        elems = [e for e in itertools.product(\n",
    "            group1_responses, group2_responses)]\n",
    "        one = [e[0] for e in elems]\n",
    "        two = [e[1] for e in elems]\n",
    "        ones.extend(one)\n",
    "        twos.extend(two)\n",
    "\n",
    "# calculate each bert score only once! (you can map the scores later to \"correspondences\" df)\n",
    "correspondences = pd.DataFrame({'ones': ones, 'twos': twos})\n",
    "unique_correspondences = correspondences.drop_duplicates()\n",
    "\n",
    "print(len(correspondences), len(unique_correspondences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cfea3396e54fed9b34968c4a915009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1f2628f1ac4c42b1917cd40ede1f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1910.27 seconds, 1352.61 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "unique_ones = unique_correspondences.ones.tolist()\n",
    "unique_twos = unique_correspondences.twos.tolist()\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "P_scores, R_scores, F1_scores = score(\n",
    "    unique_ones, unique_twos, lang='zh', verbose=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x1/rg74ttxx7bb0_2z1wxpb2z940000gn/T/ipykernel_2101/2612561919.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_correspondences['P_score'] = P_scores_list\n",
      "/var/folders/x1/rg74ttxx7bb0_2z1wxpb2z940000gn/T/ipykernel_2101/2612561919.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_correspondences['R_score'] = R_scores_list\n",
      "/var/folders/x1/rg74ttxx7bb0_2z1wxpb2z940000gn/T/ipykernel_2101/2612561919.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_correspondences['F1_score'] = F1_scores_list\n"
     ]
    }
   ],
   "source": [
    "# Assuming P_scores, R_scores, and F1_scores are tensors or similar structures, convert them to lists\n",
    "P_scores_list = P_scores.tolist()\n",
    "R_scores_list = R_scores.tolist()\n",
    "F1_scores_list = F1_scores.tolist()\n",
    "\n",
    "# Add these lists as new columns in the DataFrame\n",
    "unique_correspondences['P_score'] = P_scores_list\n",
    "unique_correspondences['R_score'] = R_scores_list\n",
    "unique_correspondences['F1_score'] = F1_scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these lists as new columns in the DataFrame\n",
    "unique_correspondences.to_csv(\"qwen_bert_unique_correspondences.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baidu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "baidu = baidu[baidu.suggestion_starts_with_query == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['category', 'group', 'group_English', 'querys', 'template_name',\n",
       "       'response', 'group_in_response', 'suggestion_starts_with_query',\n",
       "       'response_values', 'aliyun_sentiment', 'response_values_english',\n",
       "       'word_embedding_response_values', 'combine_query_response',\n",
       "       'value_alignment_response_qwen',\n",
       "       'value_alignment_response_qwen_english',\n",
       "       'value_alignment_response_ernie',\n",
       "       'value_alignment_response_ernie_english'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baidu.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2271491 230247\n"
     ]
    }
   ],
   "source": [
    "# Create similarity dataframe for ernie groups\n",
    "\n",
    "\n",
    "ones = []\n",
    "twos = []\n",
    "\n",
    "# all the combinations for all the possible groups\n",
    "for index, row in similarity_df.iterrows():\n",
    "    print(index, end='\\r')\n",
    "    group1_responses = baidu[baidu['group_English'] ==\n",
    "                             row['group1']]['response_values'].dropna().tolist()\n",
    "    group2_responses = baidu[baidu['group_English'] ==\n",
    "                            row['group2']]['response_values'].dropna().tolist()\n",
    "    if group1_responses and group2_responses:\n",
    "        elems = [e for e in itertools.product(\n",
    "            group1_responses, group2_responses)]\n",
    "        one = [e[0] for e in elems]\n",
    "        two = [e[1] for e in elems]\n",
    "        ones.extend(one)\n",
    "        twos.extend(two)\n",
    "\n",
    "# calculate each bert score only once! (you can map the scores later to \"correspondences\" df)\n",
    "correspondences = pd.DataFrame({'ones': ones, 'twos': twos})\n",
    "unique_correspondences = correspondences.drop_duplicates()\n",
    "\n",
    "print(len(correspondences), len(unique_correspondences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7dde0dacc4b4a0eb4353fb585d3c960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce6cc4fffdf4c92a86e05c2f4fab6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3598 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 177.37 seconds, 1298.10 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "unique_ones = unique_correspondences.ones.tolist()\n",
    "unique_twos = unique_correspondences.twos.tolist()\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "P_scores, R_scores, F1_scores = score(\n",
    "    unique_ones, unique_twos, lang='zh', verbose=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x1/rg74ttxx7bb0_2z1wxpb2z940000gn/T/ipykernel_42484/2612561919.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_correspondences['P_score'] = P_scores_list\n",
      "/var/folders/x1/rg74ttxx7bb0_2z1wxpb2z940000gn/T/ipykernel_42484/2612561919.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_correspondences['R_score'] = R_scores_list\n",
      "/var/folders/x1/rg74ttxx7bb0_2z1wxpb2z940000gn/T/ipykernel_42484/2612561919.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_correspondences['F1_score'] = F1_scores_list\n"
     ]
    }
   ],
   "source": [
    "# Assuming P_scores, R_scores, and F1_scores are tensors or similar structures, convert them to lists\n",
    "P_scores_list = P_scores.tolist()\n",
    "R_scores_list = R_scores.tolist()\n",
    "F1_scores_list = F1_scores.tolist()\n",
    "\n",
    "# Add these lists as new columns in the DataFrame\n",
    "unique_correspondences['P_score'] = P_scores_list\n",
    "unique_correspondences['R_score'] = R_scores_list\n",
    "unique_correspondences['F1_score'] = F1_scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these lists as new columns in the DataFrame\n",
    "unique_correspondences.to_csv(\n",
    "    \"baidu_bert_unique_correspondences.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gengliu/research/2024/stereotypes_in_search_engines-and-Chinese-LLMs/code'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
